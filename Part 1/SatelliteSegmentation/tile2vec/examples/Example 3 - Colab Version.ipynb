{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQVhfYMNSbE9"
   },
   "source": [
    "# Get SOTA Representation For Tiles\n",
    "In this notebook, we'll embed the training tiles and testing tiles in 5 different SOTA algorithms, evaluate their performance on 3 classifiers and then apply some visualisation to the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is intended to be run on Google Colab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4V_UxPnSbFA"
   },
   "outputs": [],
   "source": [
    "# For Step 1. Dataloader\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from time import time\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "\n",
    "# For Step 2. Embedding\n",
    "from img2vec_pytorch import Img2Vec\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from time import time\n",
    "import torch\n",
    "\n",
    "# For Step 3. Classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# For Step 4. Visualisation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LU7Xfs5XSh14"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "du-OHtKXSi6a"
   },
   "outputs": [],
   "source": [
    "DIR = '/content/drive/MyDrive/tile2vec/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIwTVz5FS3iI"
   },
   "outputs": [],
   "source": [
    "tile2vec_dir = '/tile2vec'\n",
    "sys.path.append('../')\n",
    "sys.path.append(DIR)\n",
    "from src.tilenet import make_tilenet\n",
    "from src.resnet import ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeeDKyJ_MYqk"
   },
   "source": [
    "Unzip TOA Train Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRycK3zUMcja"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/toa_train_tiles.zip > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLMEtzoESlNV"
   },
   "source": [
    "Unzip TOA Test Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0RO1MoISlBb",
    "outputId": "1dbc3346-603e-4664-8c64-12388706977d"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/toa_test_tiles.zip > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFScaDTlSnwF"
   },
   "source": [
    "Unzip Vis Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTV2ibcgS9FQ"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/cross_section_tiles.zip > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNLO5iTjSbFE"
   },
   "source": [
    "## Step 1. Loading trained/pre-trained Tile2Vec model\n",
    "In this step, we will initialize a new TileNet model and then load the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afiw10dISbFH",
    "outputId": "e95ae8cc-4358-41a1-b37b-c459506be165"
   },
   "outputs": [],
   "source": [
    "# Setting up model\n",
    "in_channels = 3\n",
    "z_dim = 512\n",
    "cuda = torch.cuda.is_available()\n",
    "tilenet = make_tilenet(in_channels=in_channels, z_dim=z_dim)\n",
    "# Use old model for now\n",
    "tilenet = ResNet18() # Comment out this line if using trained Tile2Vec\n",
    "if cuda: tilenet.cuda()\n",
    "print (cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUSBgFAbSbFI",
    "outputId": "ceb41533-6ffd-421b-88be-91da342608cf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment below line if using trained Tile2Vec model\n",
    "#model_fn = '../models/TileNet_epoch99_toa_data.ckpt'\n",
    "\n",
    "# Uncomment below line if using pre-trained Tile2Vec model\n",
    "model_fn = '../models/naip_trained.ckpt'\n",
    "checkpoint = torch.load(model_fn, map_location=torch.device('cpu'))\n",
    "tilenet.load_state_dict(checkpoint)\n",
    "tilenet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNvAcOwOSbFK"
   },
   "source": [
    "## Step 2. Embed tiles\n",
    "In this step, we'll use TileNet to embed the NAIP tiles provided in `tile2vec/data/tiles`. There are 1000 tiles in total, named `1tile.npy` through `1000tile.npy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ry8Qy9kkSbFL",
    "outputId": "827a91b1-7941-4c2c-c794-b2cbbb90a98a"
   },
   "outputs": [],
   "source": [
    "# Point to directory holding the tiles\n",
    "#tile_dir = '../data/toa_train_tiles'\n",
    "tile_dir = '../data/toa_test_tiles/'\n",
    "\n",
    "# The number of tiles in the directory\n",
    "# Train = 58498\n",
    "# Test = 19981\n",
    "n_tiles = 19981\n",
    "\n",
    "# Load the label vector\n",
    "y_test = np.load(os.path.join(tile_dir, 'y.npy'))\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTJMajy6SbFM"
   },
   "source": [
    "## Tile2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aph2GdiqSbFM",
    "outputId": "dfbc42ab-61cf-462e-97e5-f5e000a379ea"
   },
   "outputs": [],
   "source": [
    "# Embed tiles\n",
    "t0 = time()\n",
    "X_t2v = np.zeros((n_tiles, z_dim))\n",
    "for idx in range(n_tiles):\n",
    "    tile = np.load(os.path.join(tile_dir, '{}tile.npy'.format(idx))) #used to bbe idx+1 so undo if error\n",
    "    # Get first 4 ≠ 3 NAIP channels (5th is CDL mask)\n",
    "    tile = tile[:,:,:4]\n",
    "    # Rearrange to PyTorch order\n",
    "    tile = np.moveaxis(tile, -1, 0)\n",
    "    tile = np.expand_dims(tile, axis=0)\n",
    "    # Scale to [0, 1]\n",
    "    #tile = tile / 255\n",
    "    # Embed tile\n",
    "    \n",
    "    # Add 4th feature to each pixel for ResNet structure\n",
    "    new_col = np.full((1,1,51, 51), 0)\n",
    "    tile = np.append(tile, new_col, axis=1)\n",
    "    \n",
    "    tile = torch.from_numpy(tile).float()\n",
    "    tile = Variable(tile)\n",
    "    \n",
    "    if cuda: tile = tile.cuda()\n",
    "        \n",
    "    z = tilenet.encode(tile)\n",
    "    \n",
    "    if cuda: z = z.cpu()\n",
    "        \n",
    "    z = z.data.numpy()\n",
    "    X_t2v[idx,:] = z\n",
    "    \n",
    "t1 = time()\n",
    "print('Embedded {} time: {:0.3f}s'.format(n_tiles, t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28KaeJQ2SbFN"
   },
   "source": [
    "## ResNet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRRJTOrXhenC",
    "outputId": "461b66d4-d287-4d41-88b3-72d593603143"
   },
   "outputs": [],
   "source": [
    "!pip install img2vec_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222,
     "referenced_widgets": [
      "baed14db84974610bea11ce6dfd12232",
      "34663ff6a33e438385b83f69cc32a8d1",
      "452d83b0b8954583874a212c9d8080c4",
      "caf49405b16a4a6d9f6ec3167d056aa7",
      "93caabcd740d4b70a886d1d9f2f86972",
      "35a7b01de66c4bd49e97f720571754b8",
      "9ed3c877048b49eb857d0487b32cf887",
      "02ee28941d3142d5b0ccf985599ce2a2"
     ]
    },
    "id": "x2fqWb3LSbFQ",
    "outputId": "56f87d02-6e9e-4f10-b510-eb2d39462bff"
   },
   "outputs": [],
   "source": [
    "# Initialize Img2Vec with GPU\n",
    "img2vec = Img2Vec(cuda=True)\n",
    "UPSCALE = False\n",
    "t0 = time()\n",
    "X_res18 = np.zeros((n_tiles, z_dim))\n",
    "for idx in range(n_tiles):\n",
    "\n",
    "    # Read in an image\n",
    "    tile = np.load(os.path.join(tile_dir, '{}tile.npy'.format(idx)))\n",
    "\n",
    "    # Upscale image\n",
    "    if (UPSCALE):\n",
    "        x, y, z = tile.shape\n",
    "\n",
    "        tile = resize(tile, (224, 224,z))\n",
    "\n",
    "    im = Image.fromarray((tile * 255).astype(np.uint8))   \n",
    "    \n",
    "    vec = img2vec.get_vec(im, tensor=True)\n",
    "    \n",
    "    new_vec = [x for x in vec[0]]\n",
    "\n",
    "    X_res18[idx, :] = new_vec\n",
    "    \n",
    "    if (idx % 1000 == 0):\n",
    "        print (\"Progress: {:.2f}%\".format(idx/n_tiles*100))\n",
    "        \n",
    "t1 = time()\n",
    "print('Embedded {} tiles: {:0.3f}s'.format(n_tiles, t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SwVnnznSbFR"
   },
   "source": [
    "## SwAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3zLRnS9SbFS",
    "outputId": "50a70c90-5a49-492a-dc1d-25e467ee7232"
   },
   "outputs": [],
   "source": [
    "model = torch.hub.load('facebookresearch/swav', 'resnet50')\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTqtU2jHSbFS",
    "outputId": "02b17a03-46c7-4ec8-d05c-8a20576723de"
   },
   "outputs": [],
   "source": [
    "# Embed tiles\n",
    "t0 = time()\n",
    "z_dim = 1000\n",
    "UPSCALE = True\n",
    "X_SWAV = np.zeros((n_tiles, z_dim))\n",
    "for idx in range(n_tiles):\n",
    "    tile = np.load(os.path.join(tile_dir, '{}tile.npy'.format(idx))) #used to bbe idx+1 so undo if error\n",
    "    # Get first 4 ≠ 3 NAIP channels (5th is CDL mask)\n",
    "    tile = tile[:,:,:4]\n",
    "    # Rearrange to PyTorch order\n",
    "    tile = np.moveaxis(tile, -1, 0)\n",
    "    tile = np.expand_dims(tile, axis=0)\n",
    "    # Scale to [0, 1]\n",
    "    #tile = tile / 255\n",
    "    # Embed tile\n",
    "\n",
    "    # Upscale image\n",
    "    if (UPSCALE):\n",
    "\n",
    "        x, y, z, w = tile.shape\n",
    "\n",
    "        tile = resize(tile, (x,y,224,224))\n",
    "\n",
    "    tile = torch.from_numpy(tile).float()\n",
    "    tile = Variable(tile)\n",
    "    \n",
    "    if cuda: tile = tile.cuda()\n",
    "\n",
    "    z = model(tile)\n",
    "    \n",
    "    if cuda: z = z.cpu()\n",
    "        \n",
    "    z = z.data.numpy()\n",
    "    X_SWAV[idx,:] = z\n",
    "    \n",
    "    if idx%100 == 0:\n",
    "        print (\"Progress: {:.2f}\".format(idx/n_tiles * 100))\n",
    "        \n",
    "t1 = time()\n",
    "print('Embedded {} time: {:0.3f}s'.format(n_tiles, t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdbewBChSbFS"
   },
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znHT_03BSbFT",
    "outputId": "d9eb15c7-a467-4740-c8b4-76a50a28aa0e"
   },
   "outputs": [],
   "source": [
    "# Initialize Img2Vec with GPU\n",
    "img2vec = Img2Vec(model = 'alexnet', cuda=True)\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "X_alex = np.zeros((n_tiles, 4096))\n",
    "\n",
    "for idx in range(n_tiles):\n",
    "\n",
    "    # Read in an image\n",
    "    tile = np.load(os.path.join(tile_dir, '{}tile.npy'.format(idx)))\n",
    "    \n",
    "    im = Image.fromarray((tile * 255).astype(np.uint8))   \n",
    "    \n",
    "    vec = img2vec.get_vec(im, tensor=True)\n",
    "    \n",
    "    new_vec = [x for x in vec[0]]\n",
    "\n",
    "    X_alex[idx, :] = new_vec\n",
    "    \n",
    "    if (idx % 1000 == 0):\n",
    "        print (\"Progress: {:.2f}%\".format(idx/n_tiles*100))\n",
    "        \n",
    "t1 = time()\n",
    "print('Embedded {} tiles: {:0.3f}s'.format(n_tiles, t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Evaluate Performance On Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the embeddings that will be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(os.path.join('../embeddings/', 'X_SwAV_train.npy'))\n",
    "X_test = np.load(os.path.join('../embeddings/', 'X_SwAV_test.npy'))\n",
    "\n",
    "y_train = np.load(os.path.join('../embeddings/', 'y_train.npy'))\n",
    "y_test = np.load(os.path.join('../embeddings/', 'y_test.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split test set into a test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tst, X_val, y_tst, y_val = train_test_split(X_test, y_test, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine best depth for decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = list(range(1, 41))\n",
    "\n",
    "X_tst, X_val, y_tst, y_val = train_test_split(X_test, y_test, test_size=0.2)\n",
    "accs = []\n",
    "for depth in range(1,41):\n",
    "    print (depth)\n",
    "    decisionTree = DecisionTreeClassifier(random_state=0, max_depth = depth)\n",
    "\n",
    "    decisionTree.fit(X_train, y_train)\n",
    "\n",
    "    predictions = decisionTree.predict(X_val)\n",
    "    \n",
    "    correct_preds = 0\n",
    "    for i in range(0,len(predictions)):\n",
    "        if predictions[i] == y_val[i]:\n",
    "            correct_preds += 1\n",
    "\n",
    "    accs.append(correct_preds/len(predictions))\n",
    "\n",
    "print (np.where(accs == np.amax(accs)))\n",
    "print (np.amax(accs))\n",
    "\n",
    "plt.plot(depths, accs) #adds the line\n",
    "plt.ylabel('Accuracy') #xlabel\n",
    "plt.xlabel('Depth') #ylabel\n",
    "plt.title('Accuracy of Decision Tree as Depth Increases')\n",
    "plt.savefig('depthvsAccuracyDT') # Best = 7\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('../embeddings/', 'DT_accs_swav.npy'), accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best depths\n",
    "\n",
    "resnet18 = 7\n",
    "\n",
    "tile2vec (trained) = 8\n",
    "\n",
    "tile2vec (pre-trained) = 4\n",
    "\n",
    "AlexNet = 6\n",
    "\n",
    "SwAV = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print decision tree accuracy maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_alexnet = np.load(os.path.join('../embeddings/', 'DT_accs_alexnet.npy'))\n",
    "accs_resnet18 = np.load(os.path.join('../embeddings/', 'DT_accs_resnet18.npy'))\n",
    "accs_tile2vec = np.load(os.path.join('../embeddings/', 'DT_accs_tile2vec.npy'))\n",
    "accs_PTtile2vec = np.load(os.path.join('../embeddings/', 'DT_accs_pre_tile2vec.npy'))\n",
    "accs_swav = np.load(os.path.join('../embeddings/', 'DT_accs_swav.npy'))\n",
    "depths = list(range(1, 41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.plot(depths, accs_alexnet, '-gD', color=\"blue\",  mfc='yellow', markevery=[5], label=\"AlexNet\") #adds the line\n",
    "plt.plot(depths, accs_resnet18,'-gD', color=\"red\", mfc='yellow', markevery=[6], label=\"ResNet18\") #adds the line\n",
    "plt.plot(depths, accs_tile2vec,'-gD', color=\"green\", mfc='yellow',  markevery=[7], label=\"Tile2Vec\") #adds the line\n",
    "plt.plot(depths, accs_PTtile2vec,'-gD', color=\"pink\", mfc='yellow',  markevery=[3], label=\"Tile2Vec (pre-trained)\") #adds the line\n",
    "plt.plot(depths, accs_swav,'-gD', color=\"orange\", mfc='yellow',  markevery=[6], label=\"SwAV\") #adds the line\n",
    "\n",
    "plt.plot([0], [0.75], color=\"yellow\") #adds the line\n",
    "\n",
    "\n",
    "plt.ylabel('Accuracy') #xlabel\n",
    "plt.xlabel('Depth') #ylabel\n",
    "plt.savefig('depthvsAccuracyDT') # Best = 7\n",
    "plt.legend()\n",
    "plt.savefig('../figures/modelDTAccuracy.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get performance of decision tree with best hyper-parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1\n",
    "accs = np.zeros((n_trials,))\n",
    "precisions = np.zeros((n_trials,))\n",
    "recalls = np.zeros((n_trials,))\n",
    "fscores = np.zeros((n_trials,))\n",
    "for i in range(n_trials):\n",
    "    dt = DecisionTreeClassifier(random_state=0, max_depth =7)\n",
    "    dt.fit(X_train, y_train)\n",
    "    accs[i] = dt.score(X_tst, y_tst)\n",
    "    y_pred = dt.predict(X_test)\n",
    "    y_pred_prob = dtrf.predict_proba(X_test)\n",
    "    precisions[i], recalls[i], fscores[i], _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    print (\"TRIAL: {}\".format(i))\n",
    "    \n",
    "print (\"____Accuracy____\")\n",
    "print('Mean accuracy: {:0.4f}'.format(accs.mean()))\n",
    "print('Standard deviation: {:0.4f}'.format(accs.std()))\n",
    "\n",
    "print (\"____Macro Precision____\")\n",
    "print('Mean macro precision: {:0.4f}'.format(precisions.mean()))\n",
    "print('Standard deviation: {:0.4f}'.format(precisions.std()))\n",
    "\n",
    "print (\"____Macro Recall____\")\n",
    "print('Mean macro recall: {:0.4f}'.format(recalls.mean()))\n",
    "print('Standard deviation: {:0.4f}'.format(recalls.std()))\n",
    "\n",
    "print (\"____Macro F1-Score____\")\n",
    "print('Mean macro F1-Score: {:0.4f}'.format(fscores.mean()))\n",
    "print('Standard deviation: {:0.4f}'.format(fscores.std()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1\n",
    "accs = np.zeros((n_trials,))\n",
    "precisions = np.zeros((n_trials,))\n",
    "recalls = np.zeros((n_trials,))\n",
    "fscores = np.zeros((n_trials,))\n",
    "\n",
    "for i in range(n_trials):\n",
    "    \n",
    "    # Splitting data and training RF classifer\n",
    "    # Define the multinomial logistic regression model    \n",
    "    lr = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000, solver='saga', multi_class='multinomial'))\n",
    "    \n",
    "    # Fit the model on the whole dataset\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    accs[i] = lr.score(X_tst, y_tst)\n",
    "    \n",
    "    y_pred = lr.predict(X_tst)\n",
    "        \n",
    "    precisions[i], recalls[i], fscores[i], _ = precision_recall_fscore_support(y_tst, y_pred, average='macro')\n",
    "    \n",
    "    print (\"TRIAL: {}\".format(i))\n",
    "    \n",
    "print (\"____Accuracy____\")\n",
    "print('Mean accuracy: {:0.4f}'.format(accs.mean()))\n",
    "print('Standard deviation: {:0.4f}'.format(accs.std()))\n",
    "\n",
    "print (\"____Macro Precision____\")\n",
    "print('Mean macro precision: {:0.4f}'.format(precisions.mean()))\n",
    "print('Standard deviation: {:0.4f}'.format(precisions.std()))\n",
    "\n",
    "print (\"____Macro Recall____\")\n",
    "print('Mean macro recall: {:0.4f}'.format(recalls.mean()))\n",
    "print('Standard deviation: {:0.4f}'.format(recalls.std()))\n",
    "\n",
    "print (\"____Macro F1-Score____\")\n",
    "print('Mean macro F1-Score: {:0.4f}'.format(fscores.mean()))\n",
    "print('Standard deviation: {:0.4f}'.format(fscores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get best number of estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accs = []\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for estimator in n_estimators:\n",
    "    print (estimator)\n",
    "    rf = RandomForestClassifier(n_estimators=estimator, n_jobs=-1)\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    predictions = rf.predict(X_val)\n",
    "\n",
    "    correct_preds = 0\n",
    "    for i in range(0,len(predictions)):\n",
    "        if predictions[i] == y_val[i]:\n",
    "            correct_preds += 1\n",
    "\n",
    "    accs.append(correct_preds/len(predictions))\n",
    "\n",
    "print (np.where(accs == np.amax(accs)))\n",
    "print (np.amax(accs))\n",
    "\n",
    "plt.plot(n_estimators, accs) #adds the line\n",
    "plt.ylabel('Accuracy') #xlabel\n",
    "plt.xlabel('Number of Estimators') #ylabel\n",
    "plt.savefig('EstimatorsvsAccuracyRF') # Best = 7\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.plot(n_estimators, accs) #adds the line\n",
    "plt.ylabel('Accuracy') #xlabel\n",
    "plt.xlabel('Number of Estimators') #ylabel\n",
    "plt.savefig('EstimatorsvsAccuracyRF') # Best = 7\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('../embeddings/', 'RF_embs_accs_t2v.npy'), accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_alexnet = np.load(os.path.join('../embeddings/', 'RF_embs_accs_alex.npy'))\n",
    "accs_resnet18 = np.load(os.path.join('../embeddings/', 'RF_embs_accs_res18.npy'))\n",
    "accs_tile2vec = np.load(os.path.join('../embeddings/', 'RF_embs_accs_t2v.npy'))\n",
    "accs_PTtile2vec = np.load(os.path.join('../embeddings/', 'RF_embs_accs_pre_t2v.npy'))\n",
    "accs_swav = np.load(os.path.join('../embeddings/', 'RF_embs_accs_swav.npy'))\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.plot(n_estimators, accs_alexnet, '-gD', color=\"blue\",  mfc='yellow', markevery=[7], label=\"AlexNet\") #adds the line\n",
    "plt.plot(n_estimators, accs_resnet18,'-gD', color=\"red\", mfc='yellow', markevery=[5], label=\"ResNet18\") #adds the line\n",
    "plt.plot(n_estimators, accs_tile2vec,'-gD', color=\"green\", mfc='yellow',  markevery=[8], label=\"Tile2Vec\") #adds the line\n",
    "plt.plot(n_estimators, accs_PTtile2vec,'-gD', color=\"pink\", mfc='yellow',  markevery=[8], label=\"Tile2Vec (pre-trained)\") #adds the line\n",
    "plt.plot(n_estimators, accs_swav,'-gD', color=\"orange\", mfc='yellow',  markevery=[6], label=\"SwAV\") #adds the line\n",
    "\n",
    "plt.plot([0], [0.75], color=\"yellow\") #adds the line\n",
    "\n",
    "\n",
    "plt.ylabel('Accuracy') #xlabel\n",
    "plt.xlabel('Depth') #ylabel\n",
    "plt.savefig('depthvsAccuracyDT') # Best = 7\n",
    "plt.legend()\n",
    "plt.savefig('../figures/modelRFEstsAccuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get optimal max depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = np.linspace(1, 41, 41, endpoint=True)\n",
    "\n",
    "accs = []\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    \n",
    "    print (depth)\n",
    "    \n",
    "    rf = RandomForestClassifier(max_depth=depth, n_jobs=-1)\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    predictions = rf.predict(X_val)\n",
    "\n",
    "    correct_preds = 0\n",
    "    for i in range(0,len(predictions)):\n",
    "        if predictions[i] == y_val[i]:\n",
    "            correct_preds += 1\n",
    "\n",
    "    accs.append(correct_preds/len(predictions))\n",
    "\n",
    "print (np.where(accs == np.amax(accs)))\n",
    "print (np.amax(accs))\n",
    "\n",
    "plt.plot(max_depths, accs) #adds the line\n",
    "plt.ylabel('Accuracy') #xlabel\n",
    "plt.xlabel('Depth') #ylabel\n",
    "plt.savefig('DepthvsAccuracyRF') # Best = 7\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('../embeddings/', 'RF_depths_accs_alex.npy'), accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_alexnet = np.load(os.path.join('../embeddings/', 'RF_depths_accs_alex.npy'))\n",
    "accs_resnet18 = np.load(os.path.join('../embeddings/', 'RF_depths_accs_res18.npy'))\n",
    "accs_tile2vec = np.load(os.path.join('../embeddings/', 'RF_depths_accs_tile2vec.npy'))\n",
    "accs_PTtile2vec = np.load(os.path.join('../embeddings/', 'RF_depths_accs_pre_t2v.npy'))\n",
    "accs_swav = np.load(os.path.join('../embeddings/', 'RF_depths_accs_swav.npy'))\n",
    "depths = list(range(1, 42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.plot(depths, accs_alexnet, '-gD', color=\"blue\",  mfc='yellow', markevery=[27], label=\"AlexNet\") #adds the line\n",
    "plt.plot(depths, accs_resnet18,'-gD', color=\"red\", mfc='yellow', markevery=[19], label=\"ResNet18\") #adds the line\n",
    "plt.plot(depths, accs_tile2vec,'-gD', color=\"green\", mfc='yellow',  markevery=[22], label=\"Tile2Vec\") #adds the line\n",
    "plt.plot(depths, accs_PTtile2vec,'-gD', color=\"pink\", mfc='yellow',  markevery=[16], label=\"Tile2Vec (pre-trained)\") #adds the line\n",
    "plt.plot(depths, accs_swav,'-gD', color=\"orange\", mfc='yellow',  markevery=[28], label=\"SwAV\") #adds the line\n",
    "\n",
    "plt.plot([0], [0.75], color=\"yellow\") #adds the line\n",
    "\n",
    "\n",
    "plt.ylabel('Accuracy') #xlabel\n",
    "plt.xlabel('Depth') #ylabel\n",
    "plt.savefig('depthvsAccuracyDT') # Best = 7\n",
    "plt.legend()\n",
    "plt.savefig('../figures/modelRFDepthAccuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                Depth     Estimators\n",
    "    Tile2Vec     23          200\n",
    "    Tile2Vec(PT) 17          200\n",
    "    ResNet18     20          32\n",
    "    AlexNet      28          100\n",
    "    SWAV         29          64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3AQMwZuSbFV"
   },
   "source": [
    "Get average performance of Random Forest using optimal hyper-parameters over 10 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "junRfZiCSbFV",
    "outputId": "4bec33c8-45b0-4e31-fecd-bb866ee9ad2b"
   },
   "outputs": [],
   "source": [
    "n_trials = 10\n",
    "accs = np.zeros((n_trials,))\n",
    "precisions = np.zeros((n_trials,))\n",
    "recalls = np.zeros((n_trials,))\n",
    "fscores = np.zeros((n_trials,))\n",
    "for i in range(n_trials):\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    accs[i] = rf.score(X_tst, y_tst)\n",
    "    y_pred = rf.predict(X_tst)\n",
    "    y_pred_prob = rf.predict_proba(X_tst)\n",
    "    precisions[i], recalls[i], fscores[i], _ = precision_recall_fscore_support(y_tst, y_pred, average='macro')\n",
    "    print (\"TRIAL: {}\".format(i))\n",
    "    \n",
    "print (\"____Accuracy____\")\n",
    "print('Mean accuracy: {:0.4f}'.format(accs.mean()))\n",
    "print('Standard deviation: {:0.4f}'.format(accs.std()))\n",
    "\n",
    "print (\"____Macro Precision____\")\n",
    "print('Mean macro precision: {:0.4f}'.format(precisions.mean()))\n",
    "print('Standard deviation: {:0.4f}'.format(precisions.std()))\n",
    "\n",
    "print (\"____Macro Recall____\")\n",
    "print('Mean macro recall: {:0.4f}'.format(recalls.mean()))\n",
    "print('Standard deviation: {:0.4f}'.format(recalls.std()))\n",
    "\n",
    "print (\"____Macro F1-Score____\")\n",
    "print('Mean macro F1-Score: {:0.4f}'.format(fscores.mean()))\n",
    "print('Standard deviation: {:0.4f}'.format(fscores.std()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the best model for future classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(full_rf, open('../models/full_rf.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UH3Vfk-zSbFZ"
   },
   "source": [
    "## Step 4. Visualise The Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot t-SNE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an NDVI t-SNE plot the file ```ViewTSNE.py``` must be used to get the NDVI score of each embedding which will then be used as the ```y``` term with a green to white colour palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGP8AI3tezPn",
    "outputId": "1ee1c7fb-f7ec-45e5-9a6c-7968f9312fcd"
   },
   "outputs": [],
   "source": [
    "X_full = np.load(os.path.join('../embeddings/', 'X_SwAV_train.npy'))\n",
    "y_full = np.load(os.path.join('../embeddings/', 'X_SwAV_test.npy'))\n",
    "\n",
    "X_subset = X_full[::3]\n",
    "y_subset = y_full[::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aa9Sld1ASbFa",
    "outputId": "eb02605d-c6fb-4586-d5fb-2863eb50c5c3"
   },
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "\n",
    "tsne_results = tsne.fit_transform(X_subset)\n",
    "\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start)) # 4618 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "M3-uUKhASbFa",
    "outputId": "eec9eea1-ca6b-462d-87b2-45e42be7b4ca"
   },
   "outputs": [],
   "source": [
    "df_subset = pd.DataFrame()\n",
    "df_subset['y'] = y_subset\n",
    "df_subset['tsne-2d-one'] = tsne_results[:,0]\n",
    "df_subset['tsne-2d-two'] = tsne_results[:,1]\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", 3),\n",
    "    data=df_subset,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")\n",
    "\n",
    "plt.savefig('/content/drive/MyDrive/tile2vec/figures/tsne_toa_alexnet_full')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "kjJe_NzpSbFV"
   ],
   "name": "Example 3 - Tile2Vec features for CDL classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02ee28941d3142d5b0ccf985599ce2a2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34663ff6a33e438385b83f69cc32a8d1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35a7b01de66c4bd49e97f720571754b8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "452d83b0b8954583874a212c9d8080c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35a7b01de66c4bd49e97f720571754b8",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_93caabcd740d4b70a886d1d9f2f86972",
      "value": 46827520
     }
    },
    "93caabcd740d4b70a886d1d9f2f86972": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9ed3c877048b49eb857d0487b32cf887": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "baed14db84974610bea11ce6dfd12232": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_452d83b0b8954583874a212c9d8080c4",
       "IPY_MODEL_caf49405b16a4a6d9f6ec3167d056aa7"
      ],
      "layout": "IPY_MODEL_34663ff6a33e438385b83f69cc32a8d1"
     }
    },
    "caf49405b16a4a6d9f6ec3167d056aa7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02ee28941d3142d5b0ccf985599ce2a2",
      "placeholder": "​",
      "style": "IPY_MODEL_9ed3c877048b49eb857d0487b32cf887",
      "value": " 44.7M/44.7M [00:01&lt;00:00, 36.5MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
